{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv(\"dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title              author  \\\n",
       "0   0  House Dem Aide: We Didn’t Even See Comey’s Let...       Darrell Lucus   \n",
       "1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...     Daniel J. Flynn   \n",
       "2   2                  Why the Truth Might Get You Fired  Consortiumnews.com   \n",
       "3   3  15 Civilians Killed In Single US Airstrike Hav...     Jessica Purkiss   \n",
       "4   4  Iranian woman jailed for fictional unpublished...      Howard Portnoy   \n",
       "\n",
       "                                                text  label  \n",
       "0  House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1  Ever get the feeling your life circles the rou...      0  \n",
       "2  Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3  Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4  Print \\nAn Iranian woman has been sentenced to...      1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropiing the nan values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indepenedent and dependant features\n",
    "X = df.drop([\"label\"], axis = 1)\n",
    "y = df[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18285, 4) (18285,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocabulary size\n",
    "voc_size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preprocessing\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    #removing all specuial characters\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages[\"title\"][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review  = [ps.stem(word) for word in review if word not in stopwords.words(\"english\")]\n",
    "    review = \" \".join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hous dem aid even see comey letter jason chaffetz tweet',\n",
       " 'flynn hillari clinton big woman campu breitbart',\n",
       " 'truth might get fire']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_repr = [one_hot(words, voc_size) for words in corpus]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1387, 1901, 458, 3624, 2490, 3397, 2532, 910, 688, 2894],\n",
       " [1687, 703, 1606, 4612, 4779, 2154, 3713],\n",
       " [4445, 535, 4043, 4932]]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#onehot _represenation using the vocal=bulary size, each word \n",
    "#will be represented by number in between 0 and 5000\n",
    "onehot_repr[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ...  910  688 2894]\n",
      " [   0    0    0 ... 4779 2154 3713]\n",
      " [   0    0    0 ...  535 4043 4932]\n",
      " ...\n",
      " [   0    0    0 ... 1264  818 2177]\n",
      " [   0    0    0 ...  538 3110 1011]\n",
      " [   0    0    0 ... 4422 4377 4807]]\n"
     ]
    }
   ],
   "source": [
    "# this is the maxmium sentnec length allowed \n",
    "sentence_length  = 20\n",
    "embedded_docs = pad_sequences(onehot_repr, padding = \"pre\", \n",
    "                              maxlen = sentence_length)\n",
    "print(embedded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 40)            200000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               56400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 256,501\n",
      "Trainable params: 256,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# initializing the model\n",
    "#embedding_vector_feature is the length of the feature vector for each word\n",
    "embedding_vector_feature = 40\n",
    "model = Sequential()\n",
    "model.add(Embedding(voc_size, embedding_vector_feature, input_length = sentence_length))\n",
    "# one lstm layer with 100 neurons\n",
    "model.add(LSTM(100))\n",
    "#output layer\n",
    "model.add(Dense(1, activation = \"sigmoid\"))\n",
    "model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_final = np.array(embedded_docs)\n",
    "y_final = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "383/383 [==============================] - 4s 10ms/step - loss: 0.2850 - accuracy: 0.8683 - val_loss: 0.1909 - val_accuracy: 0.9208\n",
      "Epoch 2/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.1408 - accuracy: 0.9437 - val_loss: 0.1963 - val_accuracy: 0.9231\n",
      "Epoch 3/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.1066 - accuracy: 0.9601 - val_loss: 0.2322 - val_accuracy: 0.9190\n",
      "Epoch 4/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.0830 - accuracy: 0.9684 - val_loss: 0.2492 - val_accuracy: 0.9152\n",
      "Epoch 5/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.0632 - accuracy: 0.9780 - val_loss: 0.3024 - val_accuracy: 0.9110\n",
      "Epoch 6/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.0497 - accuracy: 0.9825 - val_loss: 0.3447 - val_accuracy: 0.9147\n",
      "Epoch 7/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.0405 - accuracy: 0.9865 - val_loss: 0.3365 - val_accuracy: 0.9079\n",
      "Epoch 8/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.0301 - accuracy: 0.9906 - val_loss: 0.3919 - val_accuracy: 0.9097\n",
      "Epoch 9/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.0246 - accuracy: 0.9922 - val_loss: 0.4417 - val_accuracy: 0.9084\n",
      "Epoch 10/10\n",
      "383/383 [==============================] - 3s 9ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.4345 - val_accuracy: 0.9090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4dc176848>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs =10, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addding dropout\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "## Creating model\n",
    "embedding_vector_features=40\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sentence_length))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance metrics and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3141  278]\n",
      " [ 271 2345]]\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_classes(X_test)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9090306545153273"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
